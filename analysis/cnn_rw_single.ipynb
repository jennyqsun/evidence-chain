{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from analysis_util import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "'''this script test the hypothesis whether people respond to a fixed bound\n",
    "pepole  '''\n",
    "# global setting f\n",
    "plt.rcParams.update({\"font.size\": 18})\n",
    "colors = sns.color_palette(\"deep\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "# top level dir \n",
    "# \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_DIR = \"/data/rwchain-all/round2\"\n",
    "BEH_DIR = os.path.join(PROJECT_DIR, \"rwchain-beh/data\")\n",
    "EEG_DIR = os.path.join(PROJECT_DIR, \"rwchain-eeg\")\n",
    "ALL_BEH_DIR = os.path.join(PROJECT_DIR, 'rwchain-beh', 'combined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DIR = \"/home/jenny/evidence-chain/\"\n",
    "FIG_DIR = os.path.join(CODE_DIR, \"figs/cnn_kernel_check/\")\n",
    "SAVE_DIR = os.path.join(CODE_DIR, \"simple_cnn_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(FIG_DIR):\n",
    "    # Create the directory\n",
    "    os.makedirs(FIG_DIR)\n",
    "    print(f\"Directory '{FIG_DIR}' was created.\")\n",
    "else:\n",
    "    print(f\"Directory '{FIG_DIR}' already exists.\")\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    # Create the directory\n",
    "    os.makedirs(SAVE_DIR)\n",
    "    print(f\"Directory '{SAVE_DIR}' was created.\")\n",
    "else:\n",
    "    print(f\"Directory '{SAVE_DIR}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimdur = '250'\n",
    "pos=9\n",
    "evaluate_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_subj = os.listdir(BEH_DIR)\n",
    "list_of_subj.sort()\n",
    "# list_of_subj = [list_of_subj[5]]\n",
    "\n",
    "df = pd.read_pickle(os.path.join(ALL_BEH_DIR, 'all_df_concat.pkl'))\n",
    "# organize some columsn\n",
    " \n",
    "df['key'][df['key'] == '[5]'] = 1\n",
    "df['key'][df['key'] == '[3]'] = 1\n",
    "df['key'][df['key'] == '[2]'] = 0\n",
    "df['cumsum'] = df['sequence_clean'].apply(lambda x: [sum(x[:i+1]) for i in range(len(x))])\n",
    "\n",
    "# get rid of a ;pw acc subject\n",
    "df = df[df['sid']!='s108']\n",
    "\n",
    "if stimdur == '100':\n",
    "    df = df[df['stimDur'] == 0.1]\n",
    "if stimdur == '250':\n",
    "    df = df[df['stimDur'] == 0.25]\n",
    "# df = df[df['stimDur'] != 0.5]\n",
    "# df = df[df['stimDur'] != 0.05]\n",
    "# df = df[df['stimDur'] == 0.1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the count\n",
    "df = correct_samples_by_condition(df)\n",
    "\n",
    "cumsum = np.array(df['cumsum'].to_list())\n",
    "sequence =  np.array(df['sequence_clean'].to_list())\n",
    "count = df['count_corrected'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of +1 and -1 with NaNs if the chain terminates\n",
    "\n",
    "chain_matrix = get_chain_matrix(sequence, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plot_performance_matrix(performance_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_resp = make_dataset(pos, count, chain_matrix)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataset, dataset_resp, test_size=0.3, random_state=2024\n",
    ")\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "x_train, y_train, test_size=0.25, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = get_evidence(x_train)\n",
    "# x_val = get_evidence(x_val)\n",
    "# x_test = get_evidence(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors and create datasets\n",
    "def create_dataset(x, y):\n",
    "    x_tensor = torch.Tensor(x).unsqueeze(1)  # Add channel dimension\n",
    "    y_tensor = torch.Tensor(y).unsqueeze(1)\n",
    "    y_tensor = y_tensor.type(torch.float32)  # Convert boolean labels to float for BCELoss\n",
    "    return TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(x_train, y_train)\n",
    "val_dataset = create_dataset(x_val, y_val)\n",
    "test_dataset = create_dataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 1, kernel_size=4, stride=1, padding=0)  # output is N x 16 x 5\n",
    "        self.fc1 = nn.Linear(1, 1,bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = torch.relu(x)\n",
    "        x, max_ind= torch.max(x[:,:,-2:], 2)  # Global average pooling\n",
    "        # print(max_ind)\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Loops\n",
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model.fc1.apply(clipper)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_outputs.extend(outputs.squeeze().cpu().numpy())\n",
    "            all_targets.extend(targets.squeeze().cpu().numpy())\n",
    "    return total_loss / len(dataloader), all_outputs, all_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if evaluate:\n",
    "    PATH = f'/home/jenny/evidence-chain/simple_cnn_models/best_pos{pos}_running.pkl'\n",
    "    model =CNN1D()\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 20\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss,_,_ = evaluate(model, train_loader, criterion)\n",
    "        val_loss, _, _ = evaluate(model, val_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        # if epoch % 10 ==0:\n",
    "        #     print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Evaluate AUC on the test set\n",
    "    test_loss, test_outputs, test_targets = evaluate(model, test_loader, criterion)\n",
    "    train_loss, train_outputs, train_targets = evaluate(model, train_loader, criterion)\n",
    "\n",
    "    test_auc = roc_auc_score(test_targets, test_outputs)\n",
    "    train_auc = roc_auc_score(train_targets, train_outputs)\n",
    "\n",
    "\n",
    "    print(f'Train Loss: {train_loss}')\n",
    "    print(f'Train AUC: {train_auc}')\n",
    "\n",
    "\n",
    "\n",
    "    test_auc = roc_auc_score(test_targets, test_outputs)\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test AUC: {test_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not evaluate_mode:\n",
    "    best_test_auc = 0\n",
    "    best_train_auc = 0\n",
    "    for i in range(300):\n",
    "        model = CNN1D().to(device)\n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "        # Train the model\n",
    "        num_epochs = 300\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "            val_loss, _, _ = evaluate(model, val_loader, criterion)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            # if epoch % 10 ==0:\n",
    "            #     print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "        # Evaluate AUC on the test set\n",
    "        test_loss, test_outputs, test_targets = evaluate(model, test_loader, criterion)\n",
    "        train_loss, train_outputs, train_targets = evaluate(model, train_loader, criterion)\n",
    "\n",
    "        test_auc = roc_auc_score(test_targets, test_outputs)\n",
    "        train_auc = roc_auc_score(train_targets, train_outputs)\n",
    "\n",
    "        print(f\"===================ATTEMP {i}======================\")\n",
    "\n",
    "        print(f'Train Loss: {train_loss}')\n",
    "        print(f'Train AUC: {train_auc}')\n",
    "\n",
    "\n",
    "\n",
    "        test_auc = roc_auc_score(test_targets, test_outputs)\n",
    "        print(f'Test Loss: {test_loss}')\n",
    "        print(f'Test AUC: {test_auc}')\n",
    "\n",
    "        if test_auc >best_test_auc and train_auc>best_train_auc:\n",
    "            best_test_auc = test_auc\n",
    "            best_train_auc = train_auc\n",
    "            PATH = f'/home/jenny/evidence-chain/simple_cnn_models/best_pos{pos}_running.pkl'\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            print(\"saved a better model!\\n\")\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(train_losses, label='Training Loss')\n",
    "            plt.plot(val_losses, label='Validation Loss')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bias = model.conv1.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_weights = model.fc1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in weights:\n",
    "    w = w.cpu().numpy().squeeze()\n",
    "    print(w)\n",
    "    plt.plot(np.arange(0,4),w,'o-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = weights.cpu().numpy()\n",
    "position = pos\n",
    "fc_bias = fc_weights.cpu().numpy()\n",
    "conv_bias = conv_bias.cpu().numpy()\n",
    "mydict = {}\n",
    "mykeys = [\"kernel\", \"position\",\"fc_bias\", \"conv_bias\"]\n",
    "values = [kernel, position, fc_bias, conv_bias]\n",
    "for key, value in zip(mykeys, values):\n",
    "    # Here you can add conditions or transformations\n",
    "    mydict[key] = value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the dictionary into a pickle file\n",
    "FILEPATH  = os.path.join(SAVE_DIR, f\"pos{pos}_param.pkl\")\n",
    "with open(FILEPATH, 'wb') as pickle_file:\n",
    "    pickle.dump(mydict, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f'/home/jenny/evidence-chain/simple_cnn_models/best_pos{pos},pkl'\n",
    "\n",
    "# torch.save(model.state_dict(), PATH)\n",
    "# m =CNN1D()\n",
    "# m.load_state_dict(torch.load(PATH))\n",
    "# m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(1, 1, 3, stride=2,bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([1,2,3],dtype=torch.float32).reshape(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.weight.data @ input.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array((0.4437, -0.3275, -0.3687))\n",
    "b = np.array((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
